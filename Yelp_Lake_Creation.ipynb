{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data analysis libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import  pyplot as mp\n",
    "\n",
    "#Other Utilities\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import datetime\n",
    "from dateutil import parser\n",
    "import logging\n",
    "import json\n",
    "\n",
    "#ETL processing library\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will use pyspark to process these data. Though the data can be accomodated in the main memory and can be processed using Pandas, it can grow in future and thus can become difficult to process in main memory. So, I am using pyspark for ETL processing.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local\").appName(name = \"Yelp Data Lake\").getOrCreate()\n",
    "\n",
    "#logger = logging.basicConfig(filename = 'D:/Capstone/logging.log' , filemode = 'a' , level = 'DEBUG')\n",
    "\n",
    "#Reading configuration data\n",
    "config_file_path = os.path.join(os.getcwd(), \"config.json\")\n",
    "with open(config_file_path) as config_file:\n",
    "    config_data = json.load(config_file)\n",
    "\n",
    "#Landing Zone path & File Names\n",
    "landing_zone = config_data['local_paths']['landing_zone']\n",
    "lake_path = config_data['local_paths']['lake_path']\n",
    "Business_dataset_file = \"yelp_academic_dataset_business.json\"\n",
    "Checkin_dataset_file = \"yelp_academic_dataset_checkin.json\"\n",
    "photo_dataset_file = \"yelp_academic_dataset_photo.json\"\n",
    "review_dataset_file = \"yelp_academic_dataset_review.json\"\n",
    "tip_dataset_file = \"yelp_academic_dataset_tip.json\"\n",
    "user_dataset_file = \"yelp_academic_dataset_user.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_business_dataset():\n",
    "    business_file = os.path.join(landing_zone , Business_dataset_file)\n",
    "\n",
    "    #Reading the json file in spark dataframe\n",
    "    business_df = spark.read.json(business_file)\n",
    "\n",
    "    #As we can see the json is a nested json. We want to flatten it to work on tabular data rather than the nested data. We will \n",
    "    # a seperate dataframe for the attribute column, wich will hold all the nested attributes in tabular format as well as it will\n",
    "    # hold the business_id which can later be used to join it with other datasets.\n",
    "    #Before all that we have to remove all the null values in the attributes column.\n",
    "    attribute_df = business_df.select([\"business_id\",\"attributes.*\"])\n",
    "    open_hours_df = business_df.select([\"business_id\",\"hours.*\"])\n",
    "    business_df = business_df.drop('attributes','hours')\n",
    "    \n",
    "\n",
    "    #Let's write all 3 dataframes to 3 different files. I will save the files in parquet format so that it consumes less space\n",
    "    business_df.write.parquet(os.path.join(lake_path, \"business\") , mode = 'overwrite', compression='snappy')\n",
    "    attribute_df.write.parquet(os.path.join(lake_path, \"business_attribute\"), mode = 'overwrite', compression='snappy')\n",
    "    open_hours_df.write.parquet(os.path.join(lake_path, \"opening_time\"), mode = 'overwrite', compression='snappy')\n",
    "\n",
    "    #Now that we have worked on Business files and the resultant is 3 new files. 1.) Attributes file -> containing business \n",
    "    #attributes, 2.) Open hours file -> containing the working hours of the business 3.) Business File -> having all other \n",
    "    #business details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tip_dataset():\n",
    "    tip_file = os.path.join(landing_zone , tip_dataset_file)\n",
    "    tip_df = spark.read.json(tip_file)\n",
    "\n",
    "    #Tip data files looks completely fine and should work well with database as well as pandas. Though the file contains data\n",
    "    #as old as year 2009, we will keep it as of now as it might help drawing some inference for data analyst or data scientist.\n",
    "    #And the dataset is not huge, will work well with pandas in memory computation.\n",
    "    #Let's just save it as parquet file\n",
    "\n",
    "    tip_df.write.parquet(os.path.join(lake_path , \"tip_files\"), mode='overwrite', compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_photo_dataset():\n",
    "    photo_file = os.path.join(landing_zone, photo_dataset_file)\n",
    "    photo_df = spark.read.json(photo_file)\n",
    "    #Let's fill blank values in caption column with Null\n",
    "    photo_df = photo_df.replace('',None)\n",
    "    photo_df.write.parquet(os.path.join(lake_path,'photo'), mode = 'overwrite' , compression = 'snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_review_dataset():\n",
    "    review_file = os.path.join(landing_zone, review_dataset_file)\n",
    "    review_df = spark.read.json(review_file)\n",
    "    #review_df.show()\n",
    "\n",
    "    #The dataset has reviews as old as 2004, for our analysis, we will consider only the last 5 years of data because \n",
    "    #it does not make any sense to analyse 2004 data. Last 5 year data can give us better understanding of how the business is\n",
    "    #doing in current scenarios.\n",
    "    \n",
    "    #Fetching maximum date in the dataset\n",
    "    max_date_rdd = review_df.selectExpr('max(date)').rdd.take(1)\n",
    "    max_date = datetime.date.fromisoformat(max_date_rdd[0][0])\n",
    "    \n",
    "    #Taking the date 5 years back from the max date\n",
    "    date_filter = datetime.date(max_date.year - 5, max_date.month, max_date.day)\n",
    "    \n",
    "    #Filtering last 5 years records\n",
    "    review_df = review_df.filter(review_df.date > date_filter)\n",
    "\n",
    "    #There are some special characters in the text column such as \\n \\t. We need to remove those to handle the data better.\n",
    "    \n",
    "    #Replacing special characters like \\n,\\t,\\b,\\r,\\,/,\\\",  \n",
    "    review_df = review_df.withColumn('text' , F.regexp_replace('text','[\\n\\t\\b\\r\\\\\\\\\\/\\\\\"]',''))\n",
    "\n",
    "    #Data looks ok after this. Let's write it to the file\n",
    "    review_df.write.parquet(os.path.join(lake_path , 'review'), mode = 'overwrite' , compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_checkin_dataset():   \n",
    "    checkin_file = os.path.join(landing_zone , Checkin_dataset_file)\n",
    "    checkin_df = spark.read.json(checkin_file)\n",
    "\n",
    "    #It's a nested json file. Let's first flatten it so that it can be processed later into table or other suitable structure.\n",
    "    checkin_df = checkin_df.select('business_id','time.*')\n",
    "\n",
    "    #The column sequence looks messed up. Let's change the sequence.\n",
    "    l = ['business_id']        \n",
    "    [l.append(d + '-' + str(i)) for d in ['Mon','Tue','Wed','Thu','Fri','Sat','Sun'] for i in range(24)]\n",
    "    checkin_df = checkin_df.select(l)\n",
    "\n",
    "    #Data looks ok now, Let's write to file\n",
    "    checkin_df.write.parquet(os.path.join(lake_path, 'checkin'), mode = 'overwrite' , compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_user_dataset():   \n",
    "    user_file = os.path.join(landing_zone , user_dataset_file)\n",
    "    user_df = spark.read.json(user_file)\n",
    "\n",
    "    #The dataset looks clean and in proper format. Let's just save it as it is.\n",
    "    user_df.write.parquet(os.path.join(lake_path, 'user'), mode = 'overwrite' , compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    '''process_business_dataset()\n",
    "    process_tip_dataset()\n",
    "    process_photo_dataset()\n",
    "    process_review_dataset()\n",
    "    process_checkin_dataset()'''\n",
    "    process_user_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Compression Ratio (Before Data Lake/ After Data Lake )***\n",
    "\n",
    "***Landing Zone directory size => 6.84 GB***\n",
    "\n",
    "***Lake directory size => 3.43 GB***\n",
    "\n",
    "***Almost 50% compression ration. Not the best but will work ok.***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
