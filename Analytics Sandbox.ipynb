{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sanchit\n",
      "[nltk_data]     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "import json\n",
    "import os\n",
    "findspark.init()\n",
    "\n",
    "#Pre requisite for textblob\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Global variable. Inititalising Spark session.\n",
    "\n",
    "\"\"\"\n",
    "spark = SparkSession.builder.master('local').appName(\"Data Science Sandbox\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Class to create analytics Sandbox\n",
    "\"\"\"\n",
    "class Data_Science_Sandbox:\n",
    "    \n",
    "    config = None\n",
    "    analytics_sandbox = None\n",
    "    lake_path = None\n",
    "    dirs = None\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.config = json.load(open(\"config.json\"))\n",
    "        self.analytics_sandbox = self.config['local_paths']['analytics_sandbox']\n",
    "        self.lake_path = self.config['local_paths']['lake_path']\n",
    "        self.dirs = [val[1] for val in os.walk(self.lake_path) if val[0] == self.lake_path][0]\n",
    "        print(spark)\n",
    "        \n",
    "    def join_all(self):\n",
    "        business_df = self.select_business_df()\n",
    "        review_df = self.select_review_df()\n",
    "        user_df = self.select_user_df()\n",
    "        final_df = business_df.join(review_df , (business_df.business_id == review_df.business_id) , how = 'inner').drop(review_df.business_id)\n",
    "        final_df = final_df.join(user_df , (final_df.user_id == user_df.user_id) , how = 'inner').drop(user_df.user_id)\n",
    "        #final_df.show()\n",
    "        self.write_to_file(final_df)\n",
    "        \n",
    "        \n",
    "    def write_to_file(self, df):\n",
    "        file_path = os.path.join(self.analytics_sandbox , 'Analytics_file')\n",
    "        df.coalesce(1).write.csv(file_path , mode  = 'overwrite' , header = True)\n",
    "        print(\"File saved at {}\".format(self.analytics_sandbox))\n",
    "        \n",
    "    \n",
    "    def select_business_df(self):\n",
    "        df = spark.read.parquet(os.path.join(self.lake_path , self.dirs[0]))\n",
    "        business_df = df.select(['business_id','name','categories','state','city','review_count','stars'])\n",
    "        business_df = self.rename_columns(business_df , ['name','categories','stars'] , ['business_name','business_categories','business_stars'])\n",
    "        return business_df\n",
    "    \n",
    "    def select_review_df(self):\n",
    "        df = spark.read.parquet(os.path.join(self.lake_path , self.dirs[5]))\n",
    "        review_df = df.select(['business_id','review_id','user_id','date','text','stars'])\n",
    "        review_df = self.rename_columns(review_df , ['date','stars'] , ['review_date','review_stars'])\n",
    "        #review_df.show()\n",
    "        return review_df\n",
    "    \n",
    "    def select_user_df(self):\n",
    "        df = spark.read.parquet(os.path.join(self.lake_path , self.dirs[7]))\n",
    "        user_df = df.select(['user_id','name','review_count'])\n",
    "        user_df = self.rename_columns(user_df , ['name','review_count'], ['user_name','user_review_count'])\n",
    "        return user_df\n",
    "     \n",
    "    '''\n",
    "    ###     Utillity to drop columns from dataframe    ###\n",
    "    def drop_columns(self, df , columns_to_drop = []):\n",
    "        if columns_to_drop == [] :\n",
    "            print(\"No column to drop\")\n",
    "            return df\n",
    "        else:\n",
    "            cols = ''.join([\"'\" + str(name)+\"',\" for name in columns_to_drop])[:-1]\n",
    "            df = df.drop(cols)\n",
    "            return df\n",
    "        \n",
    "    '''   \n",
    "    ###     Utility to rename column names    ####\n",
    "        \n",
    "    def rename_columns(self, df , old_col_names, new_col_names ):\n",
    "        if len(old_col_names) != len(new_col_names):\n",
    "            print(\"Old column name list and new column name list must have equal number of elements.\\nProcess failed.\\nNo column renamed.\")\n",
    "            return df\n",
    "        else:\n",
    "            for old_name, new_name in zip(old_col_names , new_col_names):\n",
    "                df = df.withColumnRenamed(old_name , new_name)\n",
    "            return df\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x000001B59A8DD400>\n",
      "File saved at D:/Capstone/Analytics_Sandbox/\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dw = Data_Science_Sandbox()\n",
    "    #dw.select_review_df()\n",
    "    dw.join_all()\n",
    "    #dw.select_business_df()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
